{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Abstractive Indonesian Text Summarization Using BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import torch \n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this project is the Indonesian News Dataset. The dataset contains articles and summary from seven news platform in Indonesia, which are Tempo, CNN Indonesia, CNBC Indonesia, Okezone, Suara, Kumparan, and JawaPos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df = df[[\"content\", \"summary\"]].dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing technique used for this project is the BART tokenizer. The BART tokenizer is a subword tokenizer used with the BART (Bidirectional and Auto-Regressive Transformer) model. It is based on Byte-Pair Encoding (BPE) and uses SentencePiece to handle tokenization. The tokenizer work as follows:\n",
    "- Step 1: Preprocessing\n",
    "    - The input text is lowercased and normalized (handles Unicode characters, punctuation, and spacing).\n",
    "    - It can process unseen words using subword tokenization.\n",
    "- Step 2: Tokenization (Subword Splitting)\n",
    "    - The tokenizer breaks words into subwords using Byte-Pair Encoding (BPE).\n",
    "    - Common words remain whole (\"hello\" â†’ [\"hello\"]), while rare words split into subwords (\"unhappiness\" â†’ [\"un\", \"happiness\"]).\n",
    "- Step 3: Convert Tokens to IDs\n",
    "    - Each token (or subword) is mapped to a unique integer ID from the vocabulary.\n",
    "    - Example:\n",
    "        - \"Hello World\"\n",
    "        - tensor([[    0,  31414,   232,     2]])\n",
    "- Step 4: Special Tokens\n",
    "    - BART uses special tokens for sequence modeling:\n",
    "        - ```<s>``` (Start of sentence)\n",
    "        - ```</s>``` (End of sentence)\n",
    "        - ```<mask>``` (Masked token for denoising pretraining)\n",
    "        - ```<pad>``` (Padding token for batching)\n",
    "- Step 5: Decoding (Reverse Tokenization)\n",
    "    - The model generates output as token IDs, which the tokenizer converts back to human-readable text.\n",
    "    - Example:\n",
    "        - tensor([[    0,  31414,   232,     2]])\n",
    "        - \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  inputs = [dialogue for dialogue in data_to_process[\"content\"]]\n",
    "\n",
    "  model_inputs = tokenizer(inputs,  max_length = max_input, padding = \"max_length\", truncation = True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process[\"summary\"], max_length = max_target, padding = \"max_length\", truncation = True)\n",
    "    \n",
    "  model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/500 [00:00<?, ? examples/s]c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1454.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_data = dataset.map(preprocess_data, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into training and testing dataset with a ratio of 80-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenize_data.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used for this project is the BART model. BART is a transformer model introduced by Facebook AI, that combines bidirectional and autoregressive transformers. BART uses encoder-decoder architecture that is essential for tasks involving sequences of events, such as summarization. The bidirectional approach allows the model to capture contextual information, understanding, and representing input text from both directions. Meanwhile, the autoregressive approach allows the model to create coherent and contextually rich abstractive summaries.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used for the model evaluation is the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    result = metric.compute(predictions = decoded_preds, references = decoded_labels, use_stemmer = True)\n",
    "\n",
    "    result = {key: value * 100 for key, value in result.items()}  \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"../models/bart\", \n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_steps = 100,\n",
    "    eval_steps = 100,    \n",
    "    logging_steps = 10,\n",
    "    warmup_steps = 500,    \n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    num_train_epochs = 3,\n",
    "    predict_with_generate = True,\n",
    "    eval_accumulation_steps = 1,\n",
    "    fp16 = True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_14320\\3307298478.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 51:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>1.714873</td>\n",
       "      <td>45.951865</td>\n",
       "      <td>29.282883</td>\n",
       "      <td>38.623196</td>\n",
       "      <td>38.608438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.376300</td>\n",
       "      <td>1.574822</td>\n",
       "      <td>47.155681</td>\n",
       "      <td>30.455470</td>\n",
       "      <td>39.674232</td>\n",
       "      <td>39.688220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.316200</td>\n",
       "      <td>1.467202</td>\n",
       "      <td>43.887727</td>\n",
       "      <td>27.782656</td>\n",
       "      <td>37.208616</td>\n",
       "      <td>37.224173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.802100</td>\n",
       "      <td>1.438544</td>\n",
       "      <td>45.717147</td>\n",
       "      <td>28.858607</td>\n",
       "      <td>39.036180</td>\n",
       "      <td>39.032322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.277900</td>\n",
       "      <td>1.447160</td>\n",
       "      <td>45.973286</td>\n",
       "      <td>30.236243</td>\n",
       "      <td>39.591039</td>\n",
       "      <td>39.567135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.095800</td>\n",
       "      <td>1.428631</td>\n",
       "      <td>43.214100</td>\n",
       "      <td>29.059542</td>\n",
       "      <td>37.423635</td>\n",
       "      <td>37.500033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.168700</td>\n",
       "      <td>1.389247</td>\n",
       "      <td>45.303456</td>\n",
       "      <td>28.846976</td>\n",
       "      <td>37.797551</td>\n",
       "      <td>37.819783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.152900</td>\n",
       "      <td>1.361648</td>\n",
       "      <td>45.427064</td>\n",
       "      <td>29.888587</td>\n",
       "      <td>39.069529</td>\n",
       "      <td>39.109448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>1.365604</td>\n",
       "      <td>46.019670</td>\n",
       "      <td>30.228271</td>\n",
       "      <td>39.764425</td>\n",
       "      <td>39.717266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.068600</td>\n",
       "      <td>1.356099</td>\n",
       "      <td>45.249108</td>\n",
       "      <td>29.163750</td>\n",
       "      <td>38.816451</td>\n",
       "      <td>38.841091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>1.340199</td>\n",
       "      <td>46.665656</td>\n",
       "      <td>30.799594</td>\n",
       "      <td>40.343948</td>\n",
       "      <td>40.242742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>1.319867</td>\n",
       "      <td>45.109325</td>\n",
       "      <td>29.989078</td>\n",
       "      <td>38.650220</td>\n",
       "      <td>38.628619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=1.344896384080251, metrics={'train_runtime': 3092.8869, 'train_samples_per_second': 0.388, 'train_steps_per_second': 0.388, 'total_flos': 1300262761267200.0, 'train_loss': 1.344896384080251, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bart\\\\tokenizer_config.json',\n",
       " '../models/bart\\\\special_tokens_map.json',\n",
       " '../models/bart\\\\vocab.json',\n",
       " '../models/bart\\\\merges.txt',\n",
       " '../models/bart\\\\added_tokens.json',\n",
       " '../models/bart\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../models/bart\")\n",
    "tokenizer.save_pretrained(\"../models/bart\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
